# -*- coding: utf-8 -*-
"""Deep Learning from scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x00bQMbII0ZOLj32iPOJugkbEkpwayLx
"""

!pip install -r requirements. txt

import numpy as np                                                             
import math

"""# Making Neural Networks from Scratch"""

class neural_network:

  def parameter_initialize(self,layer_dims):
    # in parameter_initialize function we will initialize weights and biases for each layer 
    # and store these weights ans biases in dictionary named parameter
    # basically these weights and biases are matrices and vectors respectively
  
    L = len(layer_dims)
    parameters = {}
    for i in range(1,L):
      parameters["w" + str(i)] = np.random.randn(layer_dims[i],layer_dims[i-1])# * math.sqrt(2./layer_dims[i-1])
      parameters["b" + str(i)] = np.zeros((layer_dims[i], 1))
    return parameters
  
  
  def linear_forward(self,a,w,b):
    # in linear_forward function we perform matrix multiplication
    # args :   a is activation of previous layer
    #          w,b are parameters of current layer
    # return : z which is matrix multiplication of w and a
    #          cache in which a,w,b are stored and will be used in backpropgation
    
    z = np.dot(w,a) + b
    cache = (a,w,b)
    q return z,cache


  def relu(self,z):
    # it is an activation function applied on z(w.a + b)
    # it will apply on intermediate layers

    a = np.maximum(0,z)
    cache = z
    return a,cache


  def sigmoid(self,z):
    # it is an activation function applied on z(w.a + b)
    # it will apply on last layer for binary output

    a = 1/(1 + np.exp(-z))
    cache = z
    return a,cache


  def activation_forward(self,a,w,b,activation):
    # args :   a is activation of previous layer
    #          w,b are parameters of current layer
    #          activation is a string which tells which activation function will apply on current layer 
    # return : a is activation of current layer that is output after perfroming matrix multiplication and applying activation function
    #          cache  will contain weights,biases,activation of previous layer and z

    if activation == "sigmoid":
      z,linear_cache= self.linear_forward(a,w,b)
      a,activation_cache = self.sigmoid(z)
    elif activation == "relu":
      z,linear_cache = self.linear_forward(a,w,b)
      a,activation_cache = self.relu(z)
    cache = (linear_cache,activation_cache)
    return a,cache


  def L_model_forward(self,x,parameters):
    # this is the function which will called after parameter initialization and perform all forward propgation step
    # args :   x is input features
    #      :   parameters is a dictionary containing weights and biases 
    # return : aL is predicted output
    #          caches is a list which will keep track of all cache which will use in backward propgation

    caches = []
    L = len(parameters) // 2
    a = x
    for i in range(1,L):
      a_prev = a
      a,cache = self.activation_forward(a_prev,parameters["w" + str(i)],parameters["b" + str(i)],"relu")
      caches.append(cache)
    aL,cache = self.activation_forward(a,parameters["w" + str(L)],parameters["b" + str(L)],"sigmoid")
    caches.append(cache)
    return aL,caches


  def compute_cost(self,aL,y):
    # args :   aL is output of last layer and  it is predicted value
    #          y is real output
    # we will compare predicted output and real output and find how close our model is predicting to real value
    # return : cost

    m = y.shape[1]
    cost = (-1/m) * (np.dot(y,np.log(aL.T)) + np.dot((1-y),np.log((1-aL).T)))
    return cost


  def compute_cost_with_regularization(self,aL,y,parameters,lambd):
    # it will return final cost and takes two extra inputs parameters and lambd
    # args :   aL is output of last layer and  it is predicted value
    #          y is real output
    #          parameters is a dictionary containing weights and biases
    #          lambd is a hyperparameter which will control the amount of regularization
    # return : cost

    L = len(parameters) // 2
    m = y.shape[1]
    cross_entropy_cost = self.compute_cost(aL,y)
    sum = 0
    for i in range(1,L+1):
      sum = sum + np.sum(np.square(parameters["w" + str(i)]))
    L2_regularization_cost = (lambd/(2*m)) * (sum)
    cost = cross_entropy_cost + L2_regularization_cost
    return cost

    def sigmoid_backward(self,cache,da):
      # it will perform only for lat layer because that was sigmoid
      # args :   cache contains activation_cache that is z
      #          da is derivative of cost with respect to aL(output)
      # return : dz is derivative of cost with respect to z(matrix multiplication of weights and previous activation performed during forward propgation)
      z = cache
      s = 1/(1 + np.exp(-z))
      dz = da * s * (1-s)
      return dz

  def relu_backward(self,cache,da):           
    z = cache
    dz = np.array(da,copy = True)
    dz[z<=0] = 0
    return dz

  def linear_backward(self,dz,cache,lambd):
    a_prev,w,b = cache
    m = a_prev.shape[1]
    dw = (1/m) * ((np.dot(dz,a_prev.T)) + ((lambd/m) * w)) 
    db = (1/m) * (np.sum(dz, axis = 1, keepdims = True))
    da_prev = np.dot(w.T,dz)
    return da_prev,dw,db


  def activation_backward(self,da,cache,lambd,activation):
    linear_cache, activation_cache = cache
    if activation == "sigmoid":
      dz = self.sigmoid_backward(activation_cache,da)
    elif activation == "relu":
      dz = self.relu_backward(activation_cache,da)
    da_prev,dw,db = self.linear_backward(dz,linear_cache,lambd)
    return da_prev,dw,db

  def L_model_backward(self,aL,caches,y,lambd):
    # args :  aL is predicted output of neural network
    #         caches is dictionary containing values which will use in backward propgation
    #         y is real output
    #         lambd is regularization hyperparameter
    # return: grads is a dictionary containing dw,db to update parameters

    grads = {}
    L = len(caches)
    m = aL.shape[1]
    y = y.reshape(aL.shape)
    daL = -(np.divide(y,aL) - (np.divide((1-y),(1-aL))))
    current_cache = caches[L-1]
    grads["da" + str(L-1)],grads["dw" + str(L)],grads["db" + str(L)] = self.activation_backward(daL,current_cache,lambd,"sigmoid")
    for i in reversed(range(L-1)):
      current_cache = caches[i]
      da_prev_temp,dw_temp,db_temp= self.activation_backward(grads["da" + str(i+1)],current_cache,lambd,"relu")
      grads["da" + str(i)] = da_prev_temp
      grads["dw" + str(i+1)] = dw_temp
      grads["db" + str(i+1)] = db_temp
    return grads


  def parameters_update(self,parameters,grads,learning_rate):
    # this function will update parameters and is controlled by a hyperparameters that is learning rate

    l = len(parameters) // 2
    for i in range(l):
      parameters["w" + str(i+1)] = parameters["w" + str(i+1)] - learning_rate * grads["dw" + str(i+1)]
      parameters["b" + str(i+1)] = parameters["b" + str(i+1)] - learning_rate * grads["db" + str(i+1)]
    return parameters 


  def train_neural_network(self,x,y,layer_dims,learning_rate,iterations,lambd,print_cost = True):
    # this function have wrapped up all functions for forward propgation,finding cost,backward propgation and parameter update 
    # and will return updated paremeters which will predict output close to real output

    costs = []
    parameters = self.parameter_initialize(layer_dims)
    for i in range(iterations):
      aL,caches = self.L_model_forward(x,parameters)
      cost = self.compute_cost_with_regularization(aL,y,parameters,lambd)
      grads = self.L_model_backward(aL,caches,y,lambd)
      parameters = self.parameters_update(parameters,grads,learning_rate)
      if print_cost and i % 100 == 0:
        print ("Cost after iteration %i: %f" %(i, cost))
      if print_cost and i % 100 == 0:
        costs.append(cost)
    return parameters

#layer_dims is a list containing number of neurons in each layer ,since we have to feed input features to our neural network that's why we have  given num_features as first element of layer_dims
#it can vary depending upon number of features in our data
# our neural network have 4 layers, first layer have 6 neurons,second have 8 neurons,third have 4 neurons and last layer that is output layer have 1 neuron becuse we want a single value 0 or 1 
# and it can also vary according to our need of output 
# x_train is features matrices and y_train is output vector
num_features = 9
layer_dims = [num_features,6,8,4,1]
learning_rate = 0.0976
iterations = 3000
lambd = 0.8
model = neural_network()
new_parameters = model.train_neural_network(x_train,y_train,layer_dims,learning_rate,iterations,lambd)

